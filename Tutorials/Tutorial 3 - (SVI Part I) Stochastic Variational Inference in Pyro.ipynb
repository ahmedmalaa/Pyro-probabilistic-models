{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI Part I: Stochastic Variational Inference in Pyro\n",
    "\n",
    "**This Tutorial is adapted from [https://pyro.ai/examples/svi_part_i.html](https://pyro.ai/examples/svi_part_i.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyro has been designed with particular attention paid to supporting stochastic variational inference as a general purpose inference algorithm. In this tutorial, we will see how we go about doing variational inference in Pyro. In the two subsequent notebooks, we will apply what we learned in this tutorial to develop two widely used probabilistic models:\n",
    "\n",
    "- **Variational autoencoders**.\n",
    "- **Hidden Markov models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to assume we have already defined our model in Pyro. As a quick reminder, the model is given as a stochastic function model(\\*args, \\**kwargs), which, in the general case takes arguments. \n",
    "\n",
    "The different pieces of model() are encoded via the mapping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. observations ⟺ **pyro.sample** with the obs argument\n",
    "\n",
    "2. latent random variables ⟺ **pyro.sample**\n",
    "\n",
    "3. parameters ⟺ **pyro.param**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's establish some notation. The model has *observations* $\\boldsymbol{x}$ and *latent random variables* $\\boldsymbol{z}$ as well as parameters $\\theta$. It has the joint probability density:\n",
    "\n",
    "$p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z}) = p_{\\theta}(\\boldsymbol{x}\\,|\\,\\boldsymbol{z})\\cdot p_{\\theta}(\\boldsymbol{z})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the various probability distributions that make up $p_{\\theta}(\\boldsymbol{x}, \\boldsymbol{z})$ have the following properties:\n",
    "\n",
    "1. We can sample from each distribution.\n",
    "2. We can compute the pointwise log pdf of each distribution.\n",
    "3. Each distribution is differentiable w.r.t. the parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context our criterion for learning a good model will be **maximizing the log evidence**, i.e. we want to find the value of $\\theta$ given by\n",
    "\n",
    "$\\theta_{max} = \\arg \\max_{\\theta} p_{\\theta}(\\boldsymbol{x}).$\n",
    "\n",
    "where the log evidence $\\log(p_{\\theta}(x))$ is given by\n",
    "\n",
    "$\\log(p_{\\theta}(x)) = \\log \\int \\, p_{\\theta}(\\boldsymbol{x},\\boldsymbol{z})\\, d\\boldsymbol{z}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the general case this is a doubly difficult problem. This is because (even for a fixed $\\theta$) the integral over the latent random variables $\\boldsymbol{z}$ is often intractable. Furthermore, even if we know how to calculate the log evidence for all values of $\\theta$, maximizing the log evidence as a function of $\\theta$ will in general be a difficult non-convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to finding $θ_{max}$, we would like to calculate the posterior over the latent variables $\\boldsymbol{z}$:\n",
    "\n",
    "$p_{\\theta_{max}}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x}) = \\frac{p_{\\theta_{max}}(\\boldsymbol{x},\\,\\boldsymbol{z})}{\\int p_{\\theta_{max}}(\\boldsymbol{x},\\,\\boldsymbol{z}) d\\boldsymbol{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the denominator of this expression is the (usually intractable) evidence. Variational inference offers a scheme for finding $\\theta_{max}$ and computing an approximation to the posterior $p_{\\theta_{max}}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$. Let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that we introduce a parameterized distribution $q_{\\varphi}(\\boldsymbol{z})$, where $\\varphi$ are known as the **variational parameters**. This distribution is called the **variational distribution** in much of the literature, and in the context of Pyro it's called **the guide** (Guide = variational distribution). The guide will serve as an approximation to the posterior distribution $p_{\\theta_{max}}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the model, the guide is encoded as a stochastic function **guide()** that contains **pyro.sample** and **pyro.param** statements. It does not contain observed data, since the guide needs to be a properly normalized distribution. Note that Pyro enforces that **model()** and **guide()** have the same call signature, i.e. both callables should take the same arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the guide is an approximation to the posterior $p_{\\theta_{max}}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$, the guide needs to provide a valid joint probability density over all the latent random variables in the model. Recall that when random variables are specified in Pyro with the primitive statement **pyro.sample()** the first argument denotes the name of the random variable. These names will be used to align the random variables in the model and guide. \n",
    "\n",
    "To be very explicit, if the model contains a random variable $z_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    \n",
    "    pyro.sample(\"z_1\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the guide needs to have a matching sample statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guide():\n",
    "    \n",
    "    pyro.sample(\"z_1\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distributions used in the two cases can be different, but the names must line-up 1-to-1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've specified a guide (we give some explicit examples below), we're ready to proceed to inference. Learning will be setup as an optimization problem where each iteration of training takes a step in $\\theta-\\varphi$ space that moves the guide closer to the exact posterior. To do this we need to define an appropriate objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence Lower Bound (ELBO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
