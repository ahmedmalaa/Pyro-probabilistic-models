{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders \n",
    "\n",
    "**This Tutorial is adapted from [https://pyro.ai/examples/vae.html](https://pyro.ai/examples/vae.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational autoencoder (VAE) is arguably the simplest deep probabilistic model. Note that VAE isn't a model as such — rather the VAE is a particular setup for doing variational inference for a certain class of models. The class of models is quite broad: basically any (unsupervised) density estimator with latent random variables. The basic structure of such a model is depicted in the Figure below.\n",
    "\n",
    "<img src=\"images/vae_model.png\" width=\"280\" height=\"280\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have depicted the structure of the kind of model we are interested in as a graphical model. We have $N$ observed datapoints $\\{\\boldsymbol{x}_i\\}$. Each datapoint is generated by a (local) latent random variable $\\boldsymbol{z}_i$. There is also a parameter $\\theta$, which is global in the sense that all the datapoints depend on it (which is why it is drawn outside the plate). Note that since $\\theta$ is a parameter, it's not something we're being Bayesian about. **What's of particular importance here is that we allow for each $\\boldsymbol{x}_i$ to depend on $\\boldsymbol{z}_i$ in a complex, non-linear way.** In practice this dependency will be parameterized by a (deep) neural network with parameters $\\theta$. It is this non-linearity that makes inference for this class of models particularly challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this non-linear structure is also one reason why this class of models offers a very flexible approach to modeling complex data. Indeed it's worth emphasizing that each of the components of the model can be \"reconfigured\" in a variety of different ways. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the neural network in $p_{\\theta}(\\boldsymbol{x}\\,|\\, \\boldsymbol{z})$ can be varied in all the usual ways (number of layers, type of non-linearities, number of hidden units, etc.)\n",
    "\n",
    "- we can choose observation likelihoods that suit the dataset at hand: gaussian, bernoulli, categorical, etc.\n",
    "\n",
    "- we can choose the number of dimensions in the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphical model representation is a useful way to think about the structure of the model, but it can also be fruitful to look at an explicit factorization of the joint probability density:\n",
    "\n",
    "$p({\\bf x}, {\\bf z}) = \\prod_{i=1}^N p_\\theta({\\bf x}_i \\,|\\, {\\bf z}_i) \\, p({\\bf z}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $p(\\boldsymbol{x},\\boldsymbol{z})$ breaks up into a product of terms like this makes it clear what we mean when we call $\\boldsymbol{z}_i$ a local random variable. For any particular $i$, only the single datapoint $\\boldsymbol{x}_i$ depends on $\\boldsymbol{z}_i$. As such the $\\boldsymbol{z}_i$ describe local structure, i.e. structure that is private to each data point. This factorized structure also means that we can do subsampling during the course of learning. As such this sort of model is amenable to the large data setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the job of the guide is to \"guess\" good values for the latent random variables — good in the sense that they're true to the model prior and true to the data. If we weren't making use of amortization, we would introduce variational parameters$ \\{\\lambda_i\\}$ for each datapoint $\\boldsymbol{x}_i$. These variational parameters would represent our belief about \"good\" values of $\\boldsymbol{z}_i$; for example, they could encode the mean and variance of a gaussian distribution in $\\boldsymbol{z}_i$ space. Amortization means that, rather than introducing variational parameters $\\lambda_i$, we instead learn a function that maps each $\\boldsymbol{x}_i$ to an appropriate $\\lambda_i$. Since we need this function to be flexible, we parameterize it as a neural network. We thus end up with a parameterized family of distributions over the latent $\\boldsymbol{z}$ space that can be instantiated for all $N$ datapoint $\\boldsymbol{x}_i$.\n",
    "\n",
    "<img src=\"images/vae_guide.png\",width=\"280\",height=\"280\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the guide $q_{\\varphi}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$ is parameterized by a global parameter $\\varphi$ shared by all the datapoints. The goal of inference will be to find \"good\" values for $\\theta$ and $\\varphi$ so that two conditions are satisfied:\n",
    "\n",
    "- the log evidence $\\log p_{\\theta}(\\boldsymbol{x})$ is large. this means our model is a good fit to the data\n",
    "- the guide $q_{\\varphi}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$ provides a good approximation to the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can zoom out and consider the high level structure of our setup. For concreteness, let's suppose the $\\{\\boldsymbol{x}_i\\}$ are images so that the model is a generative model of images. Once we have learned a good value of $\\theta$ we can generate images from the model as follows:\n",
    "\n",
    "- sample $\\boldsymbol{z}$ according to the prior $p(\\boldsymbol{x})$.\n",
    "- sample $\\boldsymbol{x}$ according to the likelihood $p_{\\theta}(\\boldsymbol{x}\\,|\\,\\boldsymbol{z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is being represented by a latent code $\\boldsymbol{z}$ and that code gets mapped to images using the likelihood, which depends on the $\\theta$ we've learned. This is why the likelihood is often called the decoder in this context: its job is to decode $\\boldsymbol{z}$ into $\\boldsymbol{x}$. Note that since this is a probabilistic model, there is uncertainty about the $\\boldsymbol{z}$ that encodes a given datapoint $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've learned good values for $\\theta$ and $\\varphi$ we can also go through the following exercise.\n",
    "\n",
    "- We start with a given image $\\boldsymbol{x}$.\n",
    "- Using our guide we encode it as $\\boldsymbol{z}$.\n",
    "- Using the model likelihood we decode $\\boldsymbol{z}$ and get a reconstructed image $\\boldsymbol{x}_{reco}$.\n",
    "\n",
    "If we've learned good values for $\\theta$ and $\\varphi$, $\\boldsymbol{x}$ and $\\boldsymbol{x}_{reco}$ should be similar. This should clarify how the word autoencoder ended up being used to describe this setup: the model is the decoder and the guide is the encoder. Together, they can be thought of as an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
