{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders \n",
    "\n",
    "**This Tutorial is adapted from [https://pyro.ai/examples/vae.html](https://pyro.ai/examples/vae.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational autoencoder (VAE) is arguably the simplest deep probabilistic model. Note that VAE isn't a model as such — rather the VAE is a particular setup for doing variational inference for a certain class of models. The class of models is quite broad: basically any (unsupervised) density estimator with latent random variables. The basic structure of such a model is depicted in the Figure below.\n",
    "\n",
    "<img src=\"images/vae_model.png\" width=\"280\" height=\"280\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have depicted the structure of the kind of model we are interested in as a graphical model. We have $N$ observed datapoints $\\{\\boldsymbol{x}_i\\}$. Each datapoint is generated by a (local) latent random variable $\\boldsymbol{z}_i$. There is also a parameter $\\theta$, which is global in the sense that all the datapoints depend on it (which is why it is drawn outside the plate). Note that since $\\theta$ is a parameter, it's not something we're being Bayesian about. **What's of particular importance here is that we allow for each $\\boldsymbol{x}_i$ to depend on $\\boldsymbol{z}_i$ in a complex, non-linear way.** In practice this dependency will be parameterized by a (deep) neural network with parameters $\\theta$. It is this non-linearity that makes inference for this class of models particularly challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this non-linear structure is also one reason why this class of models offers a very flexible approach to modeling complex data. Indeed it's worth emphasizing that each of the components of the model can be \"reconfigured\" in a variety of different ways. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the neural network in $p_{\\theta}(\\boldsymbol{x}\\,|\\, \\boldsymbol{z})$ can be varied in all the usual ways (number of layers, type of non-linearities, number of hidden units, etc.)\n",
    "\n",
    "- we can choose observation likelihoods that suit the dataset at hand: gaussian, bernoulli, categorical, etc.\n",
    "\n",
    "- we can choose the number of dimensions in the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphical model representation is a useful way to think about the structure of the model, but it can also be fruitful to look at an explicit factorization of the joint probability density:\n",
    "\n",
    "$p({\\bf x}, {\\bf z}) = \\prod_{i=1}^N p_\\theta({\\bf x}_i \\,|\\, {\\bf z}_i) \\, p({\\bf z}_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $p(\\boldsymbol{x},\\boldsymbol{z})$ breaks up into a product of terms like this makes it clear what we mean when we call $\\boldsymbol{z}_i$ a local random variable. For any particular $i$, only the single datapoint $\\boldsymbol{x}_i$ depends on $\\boldsymbol{z}_i$. As such the $\\boldsymbol{z}_i$ describe local structure, i.e. structure that is private to each data point. This factorized structure also means that we can do subsampling during the course of learning. As such this sort of model is amenable to the large data setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the job of the guide is to \"guess\" good values for the latent random variables — good in the sense that they're true to the model prior and true to the data. If we weren't making use of amortization, we would introduce variational parameters$ \\{\\lambda_i\\}$ for each datapoint $\\boldsymbol{x}_i$. These variational parameters would represent our belief about \"good\" values of $\\boldsymbol{z}_i$; for example, they could encode the mean and variance of a gaussian distribution in $\\boldsymbol{z}_i$ space. Amortization means that, rather than introducing variational parameters $\\lambda_i$, we instead learn a function that maps each $\\boldsymbol{x}_i$ to an appropriate $\\lambda_i$. Since we need this function to be flexible, we parameterize it as a neural network. We thus end up with a parameterized family of distributions over the latent $\\boldsymbol{z}$ space that can be instantiated for all $N$ datapoint $\\boldsymbol{x}_i$.\n",
    "\n",
    "<img src=\"images/vae_guide.png\" width=\"280\" height=\"280\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the guide $q_{\\varphi}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$ is parameterized by a global parameter $\\varphi$ shared by all the datapoints. The goal of inference will be to find \"good\" values for $\\theta$ and $\\varphi$ so that two conditions are satisfied:\n",
    "\n",
    "- the log evidence $\\log p_{\\theta}(\\boldsymbol{x})$ is large. this means our model is a good fit to the data\n",
    "- the guide $q_{\\varphi}(\\boldsymbol{z}\\,|\\,\\boldsymbol{x})$ provides a good approximation to the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can zoom out and consider the high level structure of our setup. For concreteness, let's suppose the $\\{\\boldsymbol{x}_i\\}$ are images so that the model is a generative model of images. Once we have learned a good value of $\\theta$ we can generate images from the model as follows:\n",
    "\n",
    "- sample $\\boldsymbol{z}$ according to the prior $p(\\boldsymbol{z})$.\n",
    "- sample $\\boldsymbol{x}$ according to the likelihood $p_{\\theta}(\\boldsymbol{x}\\,|\\,\\boldsymbol{z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is being represented by a latent code $\\boldsymbol{z}$ and that code gets mapped to images using the likelihood, which depends on the $\\theta$ we've learned. This is why the likelihood is often called the decoder in this context: its job is to decode $\\boldsymbol{z}$ into $\\boldsymbol{x}$. Note that since this is a probabilistic model, there is uncertainty about the $\\boldsymbol{z}$ that encodes a given datapoint $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've learned good values for $\\theta$ and $\\varphi$ we can also go through the following exercise.\n",
    "\n",
    "- We start with a given image $\\boldsymbol{x}$.\n",
    "- Using our guide we encode it as $\\boldsymbol{z}$.\n",
    "- Using the model likelihood we decode $\\boldsymbol{z}$ and get a reconstructed image $\\boldsymbol{x}_{reco}$.\n",
    "\n",
    "If we've learned good values for $\\theta$ and $\\varphi$, $\\boldsymbol{x}$ and $\\boldsymbol{x}_{reco}$ should be similar. This should clarify how the word autoencoder ended up being used to describe this setup: the model is the decoder and the guide is the encoder. Together, they can be thought of as an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us see how we implement a VAE in Pyro. The dataset we are going to model is MNIST, a collection of images of handwritten digits. Since this is a popular benchmark dataset, we can make use of PyTorch's convenient data loader functionalities to reduce the amount of boilerplate code we need to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for loading and batching MNIST dataset\n",
    "\n",
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    \n",
    "    root         = './data'\n",
    "    download     = True\n",
    "    trans        = transforms.ToTensor()\n",
    "    train_set    = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                              download=download)\n",
    "    \n",
    "    test_set     = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs       = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                               batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                              batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The main thing to draw attention to here is that we use **transforms.ToTensor()** to normalize the pixel intensities to the range $[0.0, 1.0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a PyTorch module that encapsulates our decoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # setup the two linear transformations used\n",
    "        self.fc1      = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21     = nn.Linear(hidden_dim, 784)\n",
    "        \n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        \n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        \n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a latent code $z$, the forward call of **Decoder** returns the parameters for a Bernoulli distribution in image space. Since each image is of size $28 \\times 28=784$, **loc_img** is of size **batch_size** $\\times 784$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a PyTorch module that encapsulates our encoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # setup the three linear transformations used\n",
    "        self.fc1      = nn.Linear(784, hidden_dim)\n",
    "        \n",
    "        # fc21 is the output layer for mean and fc22 is output layer for variance\n",
    "        self.fc21     = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22     = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        x       = x.reshape(-1, 784)\n",
    "        \n",
    "        # then compute the hidden units\n",
    "        hidden  = self.softplus(self.fc1(x))\n",
    "        \n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc   = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        \n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image $x$ the **forward** call of **Encoder** returns a mean and covariance that together parameterize a (diagonal) Gaussian distribution in latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our encoder and decoder networks in hand, we can now write down the stochastic functions that represent our model and guide. First the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model p(x|z) p(z)\n",
    "\n",
    "def model(self, x):\n",
    "    \n",
    "    # register PyTorch module \"decoder\" with Pyro\n",
    "    pyro.module(\"decoder\", self.decoder)\n",
    "    \n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        \n",
    "        # setup hyperparameters for prior p(z)\n",
    "        z_loc   = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "        z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "        \n",
    "        # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "        z       = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "        \n",
    "        # decode the latent code z\n",
    "        loc_img = self.decoder.forward(z)\n",
    "        \n",
    "        # score against actual images\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **model()** is a callable that takes in a mini-batch of images $x$ as input. This is a **torch.Tensor** of size **batch_size** $\\times$ 784."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do inside of **model()** is register the (previously instantiated) decoder module with Pyro. Note that we give it an appropriate (and unique) name. This call to **pyro.module** lets Pyro know about all the parameters inside of the decoder network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we setup the hyperparameters for our prior, which is just a unit normal gaussian distribution. Note that:\n",
    "\n",
    "- we specifically designate independence amongst the data in our mini-batch (i.e. the leftmost dimension) via **pyro.plate**. Also, note the use of **.to_event(1)** when sampling from the latent $z$. \n",
    "\n",
    "- this ensures that instead of treating our sample as being generated from a univariate normal with **batch_size* = z_dim**, we treat them as being generated from a multivariate normal distribution with diagonal covariance. As such, the log probabilities along each dimension is summed out when we evaluate **.log_prob** for a \"latent\" sample. \n",
    "\n",
    "- since we are processing an entire mini-batch of images, we need the leftmost dimension of **z_loc** and **z_scale** to equal the mini-batch size \n",
    "\n",
    "- in case we are on GPU, we use **new_zeros** and **new_ones** to ensure that newly created tensors are on the same GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sample the latent $z$ from the prior, making sure to give the random variable a unique Pyro name \"latent\". Then we pass $z$ through the decoder network, which returns **loc_img**. We then score the observed images in the mini-batch $x$ against the Bernoulli likelihood parametrized by **loc_img**. Note that we flatten $x$ so that all the pixels are in the rightmost dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it! Note how closely the flow of Pyro primitives in model follows the generative story of our model. Now we move on to the guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the guide (i.e. variational distribution) q(z|x)\n",
    "\n",
    "def guide(self, x):\n",
    "    \n",
    "    # register PyTorch module `encoder` with Pyro\n",
    "    pyro.module(\"encoder\", self.encoder)\n",
    "    \n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        \n",
    "        # use the encoder to get the parameters used to define q(z|x)\n",
    "        z_loc, z_scale = self.encoder.forward(x)\n",
    "        \n",
    "        # sample the latent code z\n",
    "        pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the model, we first register the PyTorch module we are using (namely encoder) with Pyro. We take the mini-batch of images $x$ and pass it through the encoder. Then using the parameters output by the encoder network we use the normal distribution to sample a value of the latent for each image in the mini-batch. Crucially, we use the same name for the latent random variable as we did in the model: \"latent\". Also, note the use of **pyro.plate** to designate independence of the mini-batch dimension, and **.to_event(1)** to enforce dependence on **z_dims**, exactly as we did in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the full model and guide we can move on to inference. But before that let's see how we package the model and guide in one module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    \n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder  = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder  = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "    \n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim    = z_dim\n",
    "\n",
    "    \n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        \n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            \n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc   = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            \n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z       = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            \n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder.forward(z)\n",
    "            \n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        \n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            \n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            \n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "    # define a helper function for reconstructing images\n",
    "    def reconstruct_img(self, x):\n",
    "        \n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        \n",
    "        # sample in latent space\n",
    "        z              = dist.Normal(z_loc, z_scale).sample()\n",
    "        \n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img        = self.decoder(z)\n",
    "        \n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two Modules **encoder** and **decoder** are attributes of **VAE** (which itself inherits from **nn.Module**). This has the consequence they are both automatically registered as belonging to the VAE module. So, for example, when we call **parameters()** on an instance of VAE, PyTorch will know to return all the relevant parameters. It also means that if we are running on a GPU, the call to cuda() will move all the parameters of all the (sub)modules into GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for inference. First we instantiate an instance of the **VAE** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup an instance of the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam({\"lr\": 1.0e-3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup our inference algorithm, which is going to learn good parameters for the model and guide by maximizing the ELBO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it. Now we just have to define our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        \n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train       = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    \n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mini-batch logic is handled by the **data loader**. The meat of the training loop is **svi.step(x)**. There are two things we should draw attention to here:\n",
    "\n",
    "- any arguments to step are passed to the model and the guide. consequently **model** and **guide** need to have the same call signature\n",
    "\n",
    "- **step** returns a noisy estimate of the loss. this estimate is not normalized in any way, so e.g. it scales with the size of the mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic for adding evaluation is analogous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    \n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    \n",
    "    # compute the loss over the entire test set\n",
    "    for x, _ in test_loader:\n",
    "        \n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    \n",
    "    normalizer_test       = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    \n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the only change we need to make is that we call **evaluate_loss** instead of step. This function will compute an estimate of the ELBO but won't take any gradient steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece of code we would like to highlight is the helper method **reconstruct_img** in the **VAE class**. Here, we take an image and pass it through the encoder. Then we sample in latent space using the gaussian distribution provided by the encoder. Finally we decode the latent code into an image: we return the mean vector **loc_img** instead of sampling with it. Note that since the **sample()** statement is stochastic, we'll get different draws of $z$ every time we run the **reconstruct_img** function. If we've learned a good model and guide (i.e., a good latent representation) this plurality of $z$ samples will correspond to different styles of digit writing, and the reconstructed images should exhibit an interesting variety of different styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Sample results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training corresponds to maximizing the ELBO over the training dataset. We train for 100 iterations and evaluate the ELBO for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA      = False\n",
    "\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS     = 1 if smoke_test else 100\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 193.0071\n",
      "[epoch 000] average test loss: 158.2395\n",
      "[epoch 001]  average training loss: 148.0964\n",
      "[epoch 002]  average training loss: 134.1630\n",
      "[epoch 003]  average training loss: 125.3857\n",
      "[epoch 004]  average training loss: 119.7881\n",
      "[epoch 005]  average training loss: 116.3715\n",
      "[epoch 005] average test loss: 113.9564\n",
      "[epoch 006]  average training loss: 113.9806\n",
      "[epoch 007]  average training loss: 112.2781\n",
      "[epoch 008]  average training loss: 111.0159\n",
      "[epoch 009]  average training loss: 109.9408\n",
      "[epoch 010]  average training loss: 109.1865\n",
      "[epoch 010] average test loss: 108.2264\n",
      "[epoch 011]  average training loss: 108.5013\n",
      "[epoch 012]  average training loss: 107.9597\n",
      "[epoch 013]  average training loss: 107.4520\n",
      "[epoch 014]  average training loss: 107.0464\n",
      "[epoch 015]  average training loss: 106.6734\n",
      "[epoch 015] average test loss: 106.2186\n",
      "[epoch 016]  average training loss: 106.3513\n",
      "[epoch 017]  average training loss: 106.0430\n",
      "[epoch 018]  average training loss: 105.7944\n",
      "[epoch 019]  average training loss: 105.5909\n",
      "[epoch 020]  average training loss: 105.3597\n",
      "[epoch 020] average test loss: 105.0885\n",
      "[epoch 021]  average training loss: 105.1727\n",
      "[epoch 022]  average training loss: 105.0091\n",
      "[epoch 023]  average training loss: 104.8231\n",
      "[epoch 024]  average training loss: 104.6839\n",
      "[epoch 025]  average training loss: 104.5451\n",
      "[epoch 025] average test loss: 104.3376\n",
      "[epoch 026]  average training loss: 104.3793\n",
      "[epoch 027]  average training loss: 104.2614\n",
      "[epoch 028]  average training loss: 104.1741\n",
      "[epoch 029]  average training loss: 104.0424\n",
      "[epoch 030]  average training loss: 103.9203\n",
      "[epoch 030] average test loss: 103.7126\n",
      "[epoch 031]  average training loss: 103.8457\n",
      "[epoch 032]  average training loss: 103.7456\n",
      "[epoch 033]  average training loss: 103.6626\n",
      "[epoch 034]  average training loss: 103.5812\n",
      "[epoch 035]  average training loss: 103.4679\n",
      "[epoch 035] average test loss: 103.3644\n",
      "[epoch 036]  average training loss: 103.3928\n",
      "[epoch 037]  average training loss: 103.3497\n",
      "[epoch 038]  average training loss: 103.2643\n",
      "[epoch 039]  average training loss: 103.2247\n",
      "[epoch 040]  average training loss: 103.1043\n",
      "[epoch 040] average test loss: 103.0622\n",
      "[epoch 041]  average training loss: 103.0923\n",
      "[epoch 042]  average training loss: 103.0118\n",
      "[epoch 043]  average training loss: 102.9166\n",
      "[epoch 044]  average training loss: 102.8961\n",
      "[epoch 045]  average training loss: 102.8237\n",
      "[epoch 045] average test loss: 102.8756\n",
      "[epoch 046]  average training loss: 102.7613\n",
      "[epoch 047]  average training loss: 102.7065\n",
      "[epoch 048]  average training loss: 102.6839\n",
      "[epoch 049]  average training loss: 102.6135\n",
      "[epoch 050]  average training loss: 102.5543\n",
      "[epoch 050] average test loss: 102.6291\n",
      "[epoch 051]  average training loss: 102.5092\n",
      "[epoch 052]  average training loss: 102.4592\n",
      "[epoch 053]  average training loss: 102.4157\n",
      "[epoch 054]  average training loss: 102.3389\n",
      "[epoch 055]  average training loss: 102.3510\n",
      "[epoch 055] average test loss: 102.4127\n",
      "[epoch 056]  average training loss: 102.2857\n",
      "[epoch 057]  average training loss: 102.2076\n",
      "[epoch 058]  average training loss: 102.1925\n",
      "[epoch 059]  average training loss: 102.1714\n",
      "[epoch 060]  average training loss: 102.1391\n",
      "[epoch 060] average test loss: 102.2626\n",
      "[epoch 061]  average training loss: 102.0853\n",
      "[epoch 062]  average training loss: 102.0361\n",
      "[epoch 063]  average training loss: 102.0027\n",
      "[epoch 064]  average training loss: 102.0065\n",
      "[epoch 065]  average training loss: 101.9031\n",
      "[epoch 065] average test loss: 101.9883\n",
      "[epoch 066]  average training loss: 101.8729\n",
      "[epoch 067]  average training loss: 101.8407\n",
      "[epoch 068]  average training loss: 101.8251\n",
      "[epoch 069]  average training loss: 101.8046\n",
      "[epoch 070]  average training loss: 101.7671\n",
      "[epoch 070] average test loss: 101.7312\n",
      "[epoch 071]  average training loss: 101.7142\n",
      "[epoch 072]  average training loss: 101.6912\n",
      "[epoch 073]  average training loss: 101.6553\n",
      "[epoch 074]  average training loss: 101.6354\n",
      "[epoch 075]  average training loss: 101.5560\n",
      "[epoch 075] average test loss: 101.7747\n",
      "[epoch 076]  average training loss: 101.5648\n",
      "[epoch 077]  average training loss: 101.4938\n",
      "[epoch 078]  average training loss: 101.4658\n",
      "[epoch 079]  average training loss: 101.4887\n",
      "[epoch 080]  average training loss: 101.4509\n",
      "[epoch 080] average test loss: 101.5411\n",
      "[epoch 081]  average training loss: 101.3859\n",
      "[epoch 082]  average training loss: 101.4136\n",
      "[epoch 083]  average training loss: 101.3515\n",
      "[epoch 084]  average training loss: 101.3111\n",
      "[epoch 085]  average training loss: 101.2752\n",
      "[epoch 085] average test loss: 101.7378\n",
      "[epoch 086]  average training loss: 101.2823\n",
      "[epoch 087]  average training loss: 101.2606\n",
      "[epoch 088]  average training loss: 101.1959\n",
      "[epoch 089]  average training loss: 101.1892\n",
      "[epoch 090]  average training loss: 101.1582\n",
      "[epoch 090] average test loss: 101.3068\n",
      "[epoch 091]  average training loss: 101.1351\n",
      "[epoch 092]  average training loss: 101.1184\n",
      "[epoch 093]  average training loss: 101.0739\n",
      "[epoch 094]  average training loss: 101.0353\n",
      "[epoch 095]  average training loss: 101.0312\n",
      "[epoch 095] average test loss: 101.3575\n",
      "[epoch 096]  average training loss: 101.0018\n",
      "[epoch 097]  average training loss: 100.9855\n",
      "[epoch 098]  average training loss: 100.9280\n",
      "[epoch 099]  average training loss: 100.9191\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae       = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi       = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo  = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    \n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        \n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        \n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize how the test ELBO evolves over the course of training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a1a5b75518>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.array(train_elbo), linewidth=3, label='Train loss')\n",
    "plt.plot(np.linspace(0, len(train_elbo), int(len(train_elbo)/TEST_FREQUENCY)),np.array(test_elbo), \n",
    "         linewidth=3, label='Test loss', color='r')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
